{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\tool\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import parl\n",
    "import gym\n",
    "from parl import layers\n",
    "from paddle import fluid\n",
    "from parl.utils import logger,summary\n",
    "from parl.utils import action_mapping # 将神经网络输出映射到对应的 实际动作取值范围 内\n",
    "from parl.utils import ReplayMemory # 经验回放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "    anim.save('./BipedalWalkerHardcore_result.gif', writer='imagemagick', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置超参数\n",
    "ACTION_REPEAT = 3\n",
    "ACTION_NOISE = 0.3\n",
    "ENV_SEED = 0 # ENV_SEED = 1\n",
    "ACTOR_LR = 1e-4  # Actor网络更新的 learning rate\n",
    "CRITIC_LR = 1e-4   # Critic网络更新的 learning rate\n",
    "LOG_SIG_MIN = -20.0\n",
    "LOG_SIG_MAX = 2.0\n",
    "REWARD_SCALE = 5 #REWARD_SCALE = 1.5\n",
    "\n",
    "GAMMA = 0.99        # reward 的衰减因子，一般取 0.9 到 0.999 不等\n",
    "TAU = 0.005         # target_model 跟 model 同步参数 的 软更新参数\n",
    "MEMORY_SIZE = 2e6   # replay memory的大小，越大越占用内存\n",
    "MEMORY_WARMUP_SIZE = 1e4      # replay_memory 里需要预存一些经验数据，再从里面sample一个batch的经验让agent去learn\n",
    "\n",
    "BATCH_SIZE = 256          # 每次给agent learn的数据数量，从replay memory随机里sample一批数据出来\n",
    "TRAIN_TOTAL_STEPS = 1e6   # 总训练步数\n",
    "TEST_EVERY_STEPS = 1e4    # 每个N步评估一下算法效果，episode求平均reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建model\n",
    "class ActorModel(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        hid1_size = 400\n",
    "        hid2_size = 300\n",
    "        self.fc1 = layers.fc(size=hid1_size, act='relu')\n",
    "        self.fc2 = layers.fc(size=hid2_size, act='relu')\n",
    "        self.mean_linear = layers.fc(size=act_dim)\n",
    "        self.log_std_linear = layers.fc(size=act_dim)\n",
    "    def policy(self, obs):\n",
    "        hid1 = self.fc1(obs)\n",
    "        hid2 = self.fc2(hid1)\n",
    "        means = self.mean_linear(hid2)\n",
    "        log_std = self.log_std_linear(hid2)\n",
    "        log_std = layers.clip(log_std,min = LOG_SIG_MIN,max = LOG_SIG_MAX)\n",
    "        return means, log_std\n",
    "class CriticModel(parl.Model):\n",
    "    def __init__(self):\n",
    "        hid1_size = 400\n",
    "        hid2_size = 300\n",
    "        self.fc1 = layers.fc(size=hid1_size, act='relu')\n",
    "        self.fc2 = layers.fc(size=hid2_size, act='relu')\n",
    "        self.fc3 = layers.fc(size=1, act=None)\n",
    "\n",
    "        self.fc4 = layers.fc(size=hid1_size, act='relu')\n",
    "        self.fc5 = layers.fc(size=hid2_size, act='relu')\n",
    "        self.fc6 = layers.fc(size=1, act=None)\n",
    "    def value(self, obs, act):\n",
    "        # 输入 state, action, 输出对应的Q(s,a)\n",
    "        hid1 = self.fc1(obs)\n",
    "        concat1 = layers.concat([hid1, act], axis=1)\n",
    "        Q1 = self.fc2(concat1)\n",
    "        Q1 = self.fc3(Q1)\n",
    "        Q1 = layers.squeeze(Q1, axes=[1])\n",
    "\n",
    "        hid2 = self.fc4(obs)\n",
    "        concat2 = layers.concat([hid2, act], axis=1)\n",
    "        Q2 = self.fc5(concat2)\n",
    "        Q2 = self.fc6(Q2)\n",
    "        Q2 = layers.squeeze(Q2, axes=[1])\n",
    "        return Q1, Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parl.algorithms import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipedalWalkerAgent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim):\n",
    "        assert isinstance(obs_dim, int)\n",
    "        assert isinstance(act_dim, int)\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(BipedalWalkerAgent, self).__init__(algorithm)\n",
    "\n",
    "        self.alg.sync_target(decay=0)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.sample_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.pred_act = self.alg.predict(obs)\n",
    "        with fluid.program_guard(self.sample_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.sample_act, _ = self.alg.sample(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(\n",
    "                name='act', shape=[self.act_dim], dtype='float32')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            next_obs = layers.data(\n",
    "                name='next_obs', shape=[self.obs_dim], dtype='float32')\n",
    "            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n",
    "            self.critic_cost, self.actor_cost = self.alg.learn(\n",
    "                obs, act, reward, next_obs, terminal)\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act = self.fluid_executor.run(\n",
    "            self.pred_program, feed={'obs': obs},\n",
    "            fetch_list=[self.pred_act])[0]\n",
    "        return act\n",
    "    \n",
    "    def sample(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act = self.fluid_executor.run(\n",
    "            self.sample_program,\n",
    "            feed={'obs': obs},\n",
    "            fetch_list=[self.sample_act])[0]\n",
    "        return act\n",
    "    def learn(self, obs, act, reward, next_obs, terminal):\n",
    "        feed = {\n",
    "            'obs': obs,\n",
    "            'act': act,\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs,\n",
    "            'terminal': terminal\n",
    "        }\n",
    "        [critic_cost, actor_cost] = self.fluid_executor.run(\n",
    "            self.learn_program,\n",
    "            feed=feed,\n",
    "            fetch_list=[self.critic_cost, self.actor_cost])\n",
    "        self.alg.sync_target()\n",
    "        return critic_cost[0], actor_cost[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估 agent, 跑 1 个episode，返回总reward\n",
    "def run_episode(env, agent, rpm):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action_fresh = False\n",
    "        if steps % ACTION_REPEAT == 0:\n",
    "            action_fresh = True\n",
    "        steps += 1\n",
    "        batch_obs = np.expand_dims(obs, axis=0)\n",
    "        # 重复给出同一动作，隔一定步数探索一次\n",
    "        if action_fresh:\n",
    "            if rpm.size() < MEMORY_WARMUP_SIZE:\n",
    "                action = env.action_space.sample()\n",
    "                #action = agent.sample(batch_obs.astype('float32'))\n",
    "                #action = np.squeeze(action)\n",
    "                #action = action + 0.3*np.random.rand(len(action))\n",
    "            else:\n",
    "                action = agent.sample(batch_obs.astype('float32'))\n",
    "                action = np.squeeze(action)\n",
    "                #action = action + 0.3*np.random.rand(len(action))\n",
    "        else:\n",
    "            action = last_action\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        if steps%2 == 0:\n",
    "            env.render()\n",
    "        #将掉落的reward修正为0进行训练 避免掉落对训练的影响过大\n",
    "        if done == True and reward == -100:\n",
    "            reward = -0.5\n",
    "        rpm.append(obs, action, REWARD_SCALE * reward, next_obs, done)\n",
    "\n",
    "        if rpm.size() > MEMORY_WARMUP_SIZE:\n",
    "            batch_obs, batch_action, batch_reward, batch_next_obs, batch_terminal = rpm.sample_batch(\n",
    "                BATCH_SIZE)\n",
    "            agent.learn(batch_obs, batch_action, batch_reward, batch_next_obs,\n",
    "                        batch_terminal)\n",
    "        last_action = action\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, steps\n",
    "def evaluate(env, agent):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        steps+=1\n",
    "        batch_obs = np.expand_dims(obs, axis=0)\n",
    "        action = agent.predict(batch_obs.astype('float32'))\n",
    "        action = np.squeeze(action)\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        #logger.info('reward each step:step {}, reward {}'.format(steps,reward))\n",
    "        #env.render()\n",
    "        if steps % 2 == 0:\n",
    "            frames.append(env.render(mode = 'rgb_array'))\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_dim:4,obs_dim:24，max_action:1.0\n",
      "\u001b[32m[06-27 22:16:07 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-27 22:16:08 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-27 22:16:08 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n"
     ]
    }
   ],
   "source": [
    "# 创建环境\n",
    "env = gym.make('BipedalWalkerHardcore-v3')\n",
    "env.seed(ENV_SEED)\n",
    "env.reset()\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "print('act_dim:{},obs_dim:{}，max_action:{}'.format(act_dim,obs_dim,max_action))\n",
    "actor = ActorModel(act_dim)\n",
    "critic = CriticModel()\n",
    "algorithm = SAC(\n",
    "    actor,critic,max_action=max_action, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "ckpt = 'steps_1000025.ckpt'\n",
    "agent = BipedalWalkerAgent(algorithm, obs_dim, act_dim)\n",
    "agent.restore(ckpt)\n",
    "rpm = ReplayMemory(int(MEMORY_SIZE), obs_dim, act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "while total_steps < TRAIN_TOTAL_STEPS:\n",
    "    train_reward, steps = run_episode(env, agent, rpm)\n",
    "    total_steps += steps\n",
    "    logger.info('Steps: {} Reward: {}'.format(total_steps, train_reward))\n",
    "    #ckpt = 'work/steps_{}.ckpt'.format(total_steps)\n",
    "    #agent.save(ckpt)\n",
    "    if total_steps // TEST_EVERY_STEPS >= test_flag:\n",
    "        while total_steps // TEST_EVERY_STEPS >= test_flag:\n",
    "            test_flag += 1\n",
    "            evaluate_reward = evaluate(env, agent)\n",
    "            logger.info('Steps {}, Evaluate reward: {}'.format(\n",
    "                total_steps, evaluate_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-27 22:23:15 MainThread @<ipython-input-20-be4d75dba7ea>:12]\u001b[0m Steps 279, Evaluate reward: -40.47541878484048,test_flag :1\n",
      "\u001b[32m[06-27 22:23:22 MainThread @<ipython-input-20-be4d75dba7ea>:12]\u001b[0m Steps 615, Evaluate reward: -21.326242097856564,test_flag :2\n",
      "\u001b[32m[06-27 22:23:47 MainThread @<ipython-input-20-be4d75dba7ea>:12]\u001b[0m Steps 1732, Evaluate reward: 166.64123122412332,test_flag :3\n",
      "\u001b[32m[06-27 22:23:51 MainThread @<ipython-input-20-be4d75dba7ea>:12]\u001b[0m Steps 1898, Evaluate reward: -73.60653233483403,test_flag :4\n",
      "\u001b[32m[06-27 22:24:17 MainThread @<ipython-input-20-be4d75dba7ea>:12]\u001b[0m Steps 3091, Evaluate reward: 304.5454989482416,test_flag :5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-27 22:24:18,513-WARNING: MovieWriter imagemagick unavailable; trying to use <class 'matplotlib.animation.PillowWriter'> instead.\n",
      "2020-06-27 22:24:18,515-INFO: Animation.save using <class 'matplotlib.animation.PillowWriter'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-be4d75dba7ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mTRAIN_TOTAL_STEPS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtest_flag\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mevaluate_reward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtotal_steps\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mevaluate_reward\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-76c23a34ca1a>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(env, agent)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#env.render()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tool\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tool\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_polyline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tool\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tool\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tool\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36menable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mglColor4f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mLineStyle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAttr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tool\\Anaconda3\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 开始测试\n",
    "test_flag = 0\n",
    "total_steps = 0\n",
    "add_reward = 0\n",
    "frames = []\n",
    "while total_steps < TRAIN_TOTAL_STEPS:\n",
    "    test_flag+=1\n",
    "    evaluate_reward,steps = evaluate(env, agent)\n",
    "    total_steps+=steps\n",
    "    if evaluate_reward > 0:\n",
    "        add_reward += evaluate_reward\n",
    "    logger.info('Steps {}, Evaluate reward: {},test_flag :{}'.format(total_steps, evaluate_reward,test_flag))\n",
    "    if evaluate_reward > 300:\n",
    "        display_frames_as_gif(frames)\n",
    "    frames.clear()\n",
    "    if test_flag % 100 == 0:\n",
    "        logger.info('Steps {}, 100 means Evaluate reward: {}'.format(total_steps, add_reward/100))\n",
    "        add_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
